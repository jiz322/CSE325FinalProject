{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18425"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "list2 = []\n",
    "file_data = open('amazon_review_less_than_300_chars_balanced.csv')\n",
    "for row in file_data:\n",
    "    list2.append(row)## Data pre-processing module\n",
    "    \n",
    "list3 = []\n",
    "for i in range(len(list2)):\n",
    "    list3.append((list2[i][-2],list2[i][:-3]))\n",
    "    \n",
    "random.seed(10)\n",
    "random.shuffle(list3)\n",
    "\n",
    "lenth = len(list3)\n",
    "train_list = list3[0:int(lenth*0.8)]\n",
    "test_list = list3[int(lenth*0.8):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, BertModel, AdamW, BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "        A class that holds the texts and class labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Modification: l represent train_list or test_list\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        \"\"\"\n",
    "            file_path: on MAGIC, it is either\n",
    "                        data-badassnlp/cola_public/raw/in_domain_train.tsv or\n",
    "                        data-badassnlp/cola_public/raw/in_domain_dev.tsv\n",
    "                    on Google Colab, change them to the corresponding paths.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = [s[1] for s in tqdm(l)]\n",
    "        self.labels = [int(s[0]) for s in tqdm(l)]\n",
    "        \n",
    "        \n",
    "class TextClassificationDataSet(Dataset):\n",
    "    \"\"\"\n",
    "        Define a dataset consisting of pairs of (sequence_of_word_indices, class_label).\n",
    "        class_label is either 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpora, tokenizer: BertTokenizer):\n",
    "        \"\"\"\n",
    "            This function will tokenize the sentences in self.corpora,\n",
    "            and pad the word indices properly.\n",
    "            The tokenization and padding should be done using BERT's API.\n",
    "\n",
    "                See https://huggingface.co/docs/transformers/internal/tokenization_utils\n",
    "                    for the API of tokenizer\n",
    "                See https://huggingface.co/docs/transformers/preprocessing\n",
    "                    for examples of using the API\n",
    "                Also see the BERT paper\n",
    "                    \"BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\"\n",
    "                regarding how paddings are done.\n",
    "\n",
    "            We turn the entire corpora into a big tensor in a batch, rather than\n",
    "                doing that in the __getitem__ function, as batch processing can speed up tokenization\n",
    "        \n",
    "            Then self.input_ids, self.attention_mask and self.labels\n",
    "            will be created as tensors of shape\n",
    "                torch.Size([num_sentences, max_padded_sentence_length])\n",
    "                torch.Size([num_sentences, max_padded_sentence_length])\n",
    "                torch.Size([num_sentences])\n",
    "\n",
    "            corpora: an object of the Corpora class representing some raw classified texts.\n",
    "            tokenizer: must be the BERTTokenizer loaded from the BERT/download folder\n",
    "                        in order to have the same ids for the words in any vocabulary.\n",
    "                        See the loading codes below.\n",
    "        \"\"\"\n",
    "        self.corpora = corpora\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        ### Your codes go here (30 points) ###\n",
    "\n",
    "        # Step 1 (10 points): figure out the max original sentence length with the tokenizer's padding option.\n",
    "        #           Please read the documentation of tokenizer to figure out the arguments and the return value.\n",
    "       \n",
    "        # --------------------------------------------------- \n",
    "            # Answer: \n",
    "            # By tokenizer.pad\n",
    "            # model_max_len=1000000000000000019884624838656\n",
    "        # --------------------------------------------------- \n",
    "        \n",
    "        # Step 2 (10 points): find max_length = maximum length of all training/dev sentences after padding with [CLS] and [SEP]\n",
    "        #           use the max_length to inform the tokenizer and create padded input.\n",
    "        #           Please read the documentation of tokenizer to figure out the arguments and the return value.\n",
    "\n",
    "        # --------------------------------------------------- \n",
    "\n",
    "            \n",
    "            # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "            # \"Token_type_ids\" seems to be all 0s\n",
    "            # \"Attention_mask\" seems to be all 1s (why)\n",
    "            # The \"input_ids\" seems to give a distinct value for different words. (which is nice)\n",
    "        d =  tokenizer.batch_encode_plus(corpora.sentences)\n",
    "        \n",
    "            # Here, we did not try to find maxium length. (If it will cause any trouble)\n",
    "        \n",
    "        padded_encoded_inputs = tokenizer.pad(d)\n",
    "        \n",
    "        # --------------------------------------------------- \n",
    "        \n",
    "\n",
    "\n",
    "        # Step 3 (10 points): extract the necessary tensors padded_encoded_inputs.input_ids\n",
    "        #           and padded_encoded_inputs.attention_mask\n",
    "        #           and make the self.input_ids and self.attention_mask tensors.\n",
    "        #           also make the self.labels in a tensor.\n",
    "        self.input_ids = torch.tensor(padded_encoded_inputs.input_ids)\n",
    "        self.attention_mask = torch.tensor(padded_encoded_inputs.attention_mask)\n",
    "        self.labels = torch.tensor(corpora.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpora.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            Return the idx-th of the rows the self.input_ids, self.attention_mask, self.labels in this order.\n",
    "            Don't do BERT tokenization here as that will be slow.\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, BERT_model, hidden_layer_size, num_classes):\n",
    "        \n",
    "        super(BERTClassifier, self).__init__()\n",
    "\n",
    "        # loaded pretrained model\n",
    "        self.bert = BERT_model\n",
    "        \n",
    "        # simple neural network that convert embedding of the first token to a class\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(BERT_hidden_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        The following two arguments are tensors from a mini-batch of the input_ids\n",
    "            and attention_mask returned by the BERT tokenizer.\n",
    "            \n",
    "            input_ids: a tensor of shape [batch_size, max_length]\n",
    "            attention_mask: a tensor of shape [batch_size, max_length]\n",
    "            \n",
    "            return: the logits of the sentences in the batch tensor of shape [batch_size, 1, 2]\n",
    "        \"\"\"\n",
    "        # see https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        # and https://huggingface.co/docs/transformers/main_classes/output\n",
    "        # for the return value of the forward function of BERT\n",
    "\n",
    "        ### Your codes go here (30 points) ###\n",
    "        z = self.bert.forward(input_ids, attention_mask)\n",
    "        return torch.softmax(self.classifier(z[1]).unsqueeze(1), dim=-1) \n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you check the availability of GPU when you set the device ID.\n",
    "device = torch.device(2)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "num_classes = 5\n",
    "classifier_hidden_size = 128\n",
    "\n",
    "## if using MAGIC\n",
    "BERT_PATH = '/data/badassnlp/bert-base-uncased/'\n",
    "## if using Google Colab, you need to load the bert model after it downloads the model files.\n",
    "## \n",
    "\n",
    "BERT_hidden_size = 768\n",
    "\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/badassnlp/bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training corpora ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:00<00:00, 1700127.58it/s]\n",
      "100%|██████████| 400000/400000 [00:00<00:00, 2193292.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training dataset ... \n",
      "creating training iterator ... \n",
      "creating test corpora ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 1991247.50it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 2035704.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test iterator ... \n"
     ]
    }
   ],
   "source": [
    "## 'uncased' means all words are lowered-cased before tokenization\n",
    "## 'base' means the smaller version of BERT (12 layers, 16 heads)\n",
    "## un-comment one of the following two options.\n",
    "\n",
    "# if using MAGIC, load from local BERT folder\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH, local_files_only=True)\n",
    "BERT_model = BertModel.from_pretrained(BERT_PATH, num_labels = 2, output_attentions = False, output_hidden_states = False\n",
    ").to(device)\n",
    "\n",
    "## if using Colab, load from automatically downloaded files. Downloading can take half a minute\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# BERT_model = BertModel.from_pretrained(\"bert-base-cased\", num_labels = 2, output_attentions = False, output_hidden_states = False).to(device)\n",
    "\n",
    "\n",
    "## if using MAGIC\n",
    "print(\"creating training corpora ... \")\n",
    "training_corpora = Corpora(train_list)\n",
    "print(\"creating training dataset ... \")\n",
    "training_dataset = TextClassificationDataSet(training_corpora, tokenizer)\n",
    "print(\"creating training iterator ... \")\n",
    "training_iterator = DataLoader(training_dataset, sampler = RandomSampler(training_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "## if using MAGIC\n",
    "print(\"creating test corpora ... \")\n",
    "dev_corpora = Corpora(test_list)\n",
    "print(\"creating test dataset ... \")\n",
    "dev_dataset = TextClassificationDataSet(dev_corpora, tokenizer)\n",
    "print(\"creating test iterator ... \")\n",
    "dev_iterator = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "classifier = BERTClassifier(BERT_model, classifier_hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dev_iterator)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(classifier.parameters())\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "\n",
    "num_epochs_train = 0\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "        model: an BERTClassifier object\n",
    "        iterator: a DataLoader object\n",
    "        optimizer: torch optimizer\n",
    "        criterion: the crossentropyloss\n",
    "        clip: to be used with torch.nn.utils.clip_grad_norm_\n",
    "\n",
    "        return: average loss over the training instances (sentences) in the DataLoader.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total_instances = 0\n",
    "    global num_epochs_train\n",
    "    num_epochs_train += 1\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        num_batchs += 1\n",
    "        optimizer.zero_grad() \n",
    "        tmp = optimizer.state_dict()\n",
    "        tmp[\"param_groups\"][0][\"lr\"] = 0.00002/(num_epochs_train)\n",
    "        optimizer.load_state_dict(tmp)\n",
    "\n",
    "        ### Your codes go here (10 points) ###\n",
    "\n",
    "        # Step 1 (5 points): get the tensors from this mini-batch and increase the total_instances\n",
    "        # make sure tensors are moved to GPU.\n",
    "        ids = batch[0].to(device)\n",
    "        msk = batch[1].to(device)\n",
    "        y = (batch[2]-1).to(device)\n",
    "        \n",
    "        # Step 2 (5 points): call the forward function of the model and find the output logits\n",
    "        # then calculate the loss. Then cumulate the epoch_loss\n",
    "        logits = model.forward(ids,msk)\n",
    "        loss = criterion(logits.squeeze(1), y) # if squeeze here, why unsqueeze previous\n",
    "        loss.backward()\n",
    "\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        total_instances += BATCH_SIZE\n",
    "\n",
    "    return epoch_loss / total_instances\n",
    "\n",
    "confusion_matrix = []\n",
    "num_epochs = 0\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "        Find the loss (criterion) of the model over instances in this DataLoader (iterator)\n",
    "        Same logic as the above train function, except no gradient calculation\n",
    "        and no update of the parameter. \n",
    "        \n",
    "        return: average loss over the training instances (sentences) in the DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total_instances = 0\n",
    "    \n",
    "    confusion_matrix.append(torch.zeros(5,5))\n",
    "    global num_epochs\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        ### Your codes go here (5 points) ###\n",
    "        ids = batch[0].to(device)\n",
    "        msk = batch[1].to(device)\n",
    "        y = (batch[2]-1).to(device)\n",
    "        logits = model.forward(ids,msk)\n",
    "        loss = criterion(logits.squeeze(1), y) # if squeeze here, why unsqueeze previous\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total_instances += BATCH_SIZE\n",
    "        \n",
    "\n",
    "        for i in range(len(y)):\n",
    "            row = y[i]\n",
    "            col = torch.argmax((logits.squeeze(1))[i])\n",
    "            confusion_matrix[num_epochs][row][col] += 1\n",
    "    num_epochs += 1\n",
    "\n",
    "    return epoch_loss / total_instances\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [2:12:03,  6.31it/s]\n",
      "12500it [05:04, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 137m 8s\tTrain Loss: 1.199 | Test Loss: 1.278\n",
      "epoch start:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [2:12:17,  6.30it/s]\n",
      "12500it [05:04, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 137m 22s\tTrain Loss: 1.188 | Test Loss: 1.278\n",
      "epoch start:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40658it [1:47:39,  6.33it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "4202it [11:06,  6.37it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "19782it [52:17,  6.33it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "50000it [2:12:13,  6.30it/s]\n",
      "12500it [05:04, 41.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 137m 18s\tTrain Loss: 1.172 | Test Loss: 1.277\n",
      "epoch start:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [2:13:57,  6.22it/s]\n",
      "12500it [05:04, 41.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 139m 1s\tTrain Loss: 1.165 | Test Loss: 1.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    print(\"epoch start: \", epoch)  \n",
    "    start_time = time.time()\n",
    "    training_loss = train(classifier, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    test_loss = evaluate(classifier, dev_iterator, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(classifier.state_dict(), 'best_model.pt')\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13747.,  4101.,  1436.,   349.,   331.],\n",
      "        [ 5560.,  9472.,  3783.,   565.,   498.],\n",
      "        [ 1802.,  4282., 10263.,  3023.,   541.],\n",
      "        [  219.,   377.,  2468., 12145.,  4968.],\n",
      "        [  137.,   225.,   361.,  2384., 16963.]])\n",
      "[1.3146484128952027, 1.2859456231069566, 1.279662517466545, 1.27645985683918, 1.2793248378181457, 1.2779897089242935, 1.2776662415266038, 1.2773806825733185, 1.2773248982572556, 1.2765346831417084]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix[9])\n",
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist: tensor(46478.)\n",
      "diag:  tensor(62590.)\n"
     ]
    }
   ],
   "source": [
    "distance = 0\n",
    "diagnal = 0\n",
    "\n",
    "ep = 10\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        distance+= confusion_matrix[ep-1][i][j]*abs(i-j)\n",
    "        if i == j:\n",
    "            diagnal += confusion_matrix[ep-1][i][j]\n",
    "        \n",
    "print(\"dist:\" ,distance)\n",
    "print(\"diag: \",diagnal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1, 1, 1, 2, 3, 3])\n",
      "tensor([[[4.6859e-01, 3.9175e-01, 1.3949e-01, 1.3469e-04, 4.0148e-05]],\n",
      "\n",
      "        [[1.6594e-07, 3.1954e-06, 9.9999e-01, 1.9403e-06, 5.7594e-09]],\n",
      "\n",
      "        [[1.0000e+00, 3.1152e-07, 4.0230e-08, 1.0431e-07, 3.2509e-11]],\n",
      "\n",
      "        [[2.6730e-01, 5.7599e-01, 1.5535e-01, 8.8893e-04, 4.7313e-04]],\n",
      "\n",
      "        [[3.2141e-06, 9.9999e-01, 1.9507e-06, 9.3135e-10, 3.5361e-09]],\n",
      "\n",
      "        [[8.0248e-07, 1.1253e-07, 2.6564e-06, 9.9999e-01, 3.1390e-06]],\n",
      "\n",
      "        [[1.1651e-03, 1.4594e-02, 1.3898e-01, 6.9422e-01, 1.5104e-01]],\n",
      "\n",
      "        [[1.0000e+00, 4.7848e-07, 3.6542e-08, 7.4078e-08, 3.2185e-11]]],\n",
      "       device='cuda:2', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "int(train_list[10][0])-1\n",
    "\n",
    "print(list(dev_iterator)[0][2]-1)\n",
    "\n",
    "print(classifier.forward(list(dev_iterator)[0][0].cuda(2),list(dev_iterator)[0][1].cuda(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19964, 19878, 19911, 20177, 20070]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0:58]\n",
    "count = [0,0,0,0,0]\n",
    "for i in test_list:\n",
    "    count[int(i[0])-1]+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
