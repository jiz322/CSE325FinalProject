{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18244"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 400000\n",
      "('4', \"it works great! i'm not very good with doing my hair and without any effort it's straight and looks professional!\")\n",
      "['5', '4', '3', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "list2 = []\n",
    "file_data = open('amazon_review_less_than_300_chars_balanced.csv')\n",
    "for row in file_data:\n",
    "    list2.append(row)## Data pre-processing module\n",
    "    \n",
    "list3 = []\n",
    "for i in range(len(list2)):\n",
    "    list3.append((list2[i][-2],list2[i][:-3]))\n",
    "    \n",
    "lenth = len(list3)\n",
    "train_list = list3[0:int(lenth*0.8)]\n",
    "test_list = list3[int(lenth*0.8):]\n",
    "print(len(test_list), len(train_list))\n",
    "print(train_list[3000])\n",
    "\n",
    "l = []\n",
    "for i,j in list3:\n",
    "    if i not in l:\n",
    "        l.append(i)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED=4321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "    The class holds training and test corpora.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        # word to index (1-based integers) mapping\n",
    "        self.word_index = {\"N0O0N\":0}\n",
    "        # list of reviews tuples, each of which is (sentence_list, rate),\n",
    "        self.training_reviews = []\n",
    "        # (sentence_list, rate) Same format as training_sentences\n",
    "        self.test_reviews = []\n",
    "\n",
    "        self.max_len = 0\n",
    "\n",
    "\n",
    "    # input: a tuple (reviewList, rate)\n",
    "    # todo: insert values into fields\n",
    "    # Return the list representing all index of words in a review.\n",
    "    def insert_fields(self, input):   \n",
    "        # Sentence list\n",
    "        word_indexes = []\n",
    "        for word in input:\n",
    "            if word not in self.word_index.keys():\n",
    "                self.word_index.update({word:len(self.word_index.keys())}) #No add 1 because 0 is already in\n",
    "            # find the index of this word, add to return list\n",
    "            word_indexes.append(self.word_index[word])\n",
    "        if len(word_indexes)>self.max_len:\n",
    "            self.max_len = len(word_indexes)\n",
    "        return word_indexes\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Different than P2, here we \n",
    "    def read_corpus(self, is_training):\n",
    "        if is_training is True:\n",
    "            target = train_list\n",
    "        else:\n",
    "            target = test_list\n",
    "        print(\"reading corpus ...\")\n",
    "        for rate, text in tqdm(target):\n",
    "            input = text.split(\" \")\n",
    "            tuple = (self.insert_fields(input), rate)\n",
    "            if is_training: \n",
    "                self.training_reviews.append(tuple)\n",
    "            else:\n",
    "                self.test_reviews.append(tuple)\n",
    "                    \n",
    "                \n",
    "# Inherient Dataset, convert list and int to tensors, load to GPU.\n",
    "class ReviewRateDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, review_rate_pairs): # NB: sequence_pairs is corpora.training_reviews, \n",
    "        # list of (sentence_list, rate)\n",
    "        self.review_rate_pairs = review_rate_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review_rate_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_list, rate = self.review_rate_pairs[idx] \n",
    "        return torch.tensor(sentence_list), torch.tensor(int(rate))\n",
    "\n",
    "# NB! This class will be in DataLoader function as a parameter for batch_sampler\n",
    "class SortedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "        Each sequence in a mini-batch must of the same lengths, while our sentences\n",
    "        are of various lengths.\n",
    "        We can pad the sentences to the same lengths in each mini-batch.\n",
    "        But if a short and long sentences are in the same mini-batch, more paddings\n",
    "        are needed.\n",
    "        We sort the sentences based on their lengths (in descending order)\n",
    "            and then put sentences with similar lengths in a batch to reduce the paddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "            dataset: an torch.utils.data.DataSet object containing all training sequences\n",
    "            batch_size: the number of sequences to put in a mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # The sorting and batching go within this function.      \n",
    "        self.batch_size = batch_size \n",
    "        # Sort the dataset (Based on the length of sentence.)\n",
    "        dataset.review_rate_pairs  = sorted(dataset.review_rate_pairs,key=lambda x:len(x[0]), reverse=True)\n",
    "        self.sorted_lengths = len(dataset)\n",
    "        # Batching: Split the dataset into a list of datasets\n",
    "        self.index_batches = []  \n",
    "        # -- NB: Collate function does not work, so I pad it directly.\n",
    "        for i in range(self.__len__()):\n",
    "            self.index_batches.append(padding_collate_func(ReviewRateDataset(dataset.review_rate_pairs[i*batch_size:i*batch_size+batch_size])))\n",
    "        # Now, each mini-batches is a ReviewRateDataset object\n",
    "        # If else format is needed, may change it latter.\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            return a Python iterator object that iterates the mini-batchs of\n",
    "                training data indices (not individual indices)\n",
    "        \"\"\"\n",
    "        return iter(self.index_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sorted_lengths // self.batch_size\n",
    "\n",
    "# NB! This function will be in DataLoader function as a parameter for collate_fn\n",
    "def padding_collate_func(batch):\n",
    "    \"\"\"\n",
    "        Transform pairs of input-output sequences in the batch to be of the same length using the function\n",
    "            torch.nn.utils.rnn.pad_sequence.\n",
    "        batch: An iterator and each element is a pair of (input_sequence, output_sequence).\n",
    "        For POS tagging, len(input_sequence) = len(output_sequence). But for different\n",
    "        pairs in batch, their lengths can differ.\n",
    "\n",
    "        Example: a batch of 3 pairs of input/output sequences\n",
    "                [([1,2,3],[1,1,1]), ([1,2,3,4],[2,2,2,2]), ([1,2,3,4,5],[3,3,3,3,3])]\n",
    "                Note: [] encloses tensors (not numpy arra ys)\n",
    "                \n",
    "                \n",
    "                !!!!!NB QUESTION:  it is the inner [] that encloses tensors, right?\n",
    "                Comment: Batch is an element of a Sampler (see test_p1.py, a l[0] is a batch)\n",
    "                \n",
    "                \n",
    "        return: two tensors (one for input sequence batch and another for output sequence batch).\n",
    "                These tensors are padded with zeros so that all sequences in the same batch\n",
    "                are of the same length.\n",
    "        Example: input_sequence_batch = [[1,2,3,0,0], [1,2,3,4,0], [1,2,3,4,5]],\n",
    "                 output_sequence_batch = [[1,1,1,0,0], [2,2,2,2,0], [3,3,3,3,3]]\n",
    "\n",
    "    \"\"\"\n",
    "    ### Your codes go here (5 points) ###\n",
    "    # Hint: read the article linked at the top of this cell.\n",
    "    \n",
    "    # NOTe\n",
    "    # len(batch[0][0]) == len(batch.sequence_pairs[0]) == the target value (the maximum length for each batch)\n",
    "    # I fill it might be easier to pad sequence_pairs with tuple (0,0), but let's see.\n",
    "    \n",
    "    # Doubles the memory takes by this batch, which is not good\n",
    "    new_l = []\n",
    "    rate = []\n",
    "    for i,j in batch.review_rate_pairs:\n",
    "        new_l.append( torch.tensor(i))\n",
    "        rate.append( torch.tensor(int(j)))\n",
    "    padded = pad_sequence(new_l, batch_first=True, padding_value=0)\n",
    "    #print(\"len \", len(padded), len(rate)) #BUG: NEED ZIP HERE\n",
    "    obj = ReviewRateDataset(list(zip(padded,rate)))\n",
    "    # NB: For now the outter [] is neither tensor nor list, it is an obj!\n",
    "    # Change latter if necessary\n",
    "    # Yes, let's change it to two tensors return\n",
    "    ret1 = []\n",
    "    ret2 = []\n",
    "    for i in obj:\n",
    "        ret1.append(i[0])\n",
    "        ret2.append(i[1])\n",
    "    ret1 = torch.stack(ret1)\n",
    "    ret2 = torch.stack(ret2)\n",
    "    return ret1, ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import embedding, nn\n",
    "# There is really nothing to be stored in this object.\n",
    "# -- But wait, how about self.rnn and self.fc?\n",
    "# -- NB: NOW, I assume that the nn keep weights from the inherentance,\n",
    "# -- And these functions as LSTM and FC will use these weight correctly\n",
    "class LSTMScoreAssigner(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        \"\"\"\n",
    "        :param input_dim: size of the vocabulary (number of unique tokens)\n",
    "        :param output_dim: number of unique POS tags \n",
    "        :param emb_dim: embedding dimensionality of each token\n",
    "        :param hid_dim: number of hidden neurons of a hidden state/cell\n",
    "        :param n_layers: number of RNN layers (2 for faster training)\n",
    "        :param dropout: dropout rate between 0 and 1at the embedding layer and rnn\n",
    "        :param bidirectional: 1 if use bidirectional and 0 if don't\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        vector_weights = [1.0/input_dim]*input_dim\n",
    "        self.vector_weights = nn.Parameter(torch.tensor(vector_weights, requires_grad=True))\n",
    "\n",
    "        # before output, there is a dropout (except the last layer)\n",
    "        \n",
    "        # -- Comment: this is a part of the analysis (last part of this hw)\n",
    "        # -- It feels like I have no control on this bidirectional since it is part of nn library.\n",
    "        if bidirectional == 0:\n",
    "            self.rnn = nn.LSTM(input_size = emb_dim, hidden_size = hid_dim, num_layers = n_layers, dropout=dropout)\n",
    "            self.fc = nn.Linear(hid_dim, output_dim)\n",
    "            self.num_directions = 1\n",
    "        elif bidirectional == 1:\n",
    "            self.rnn = nn.LSTM(input_size = emb_dim, hidden_size = hid_dim, num_layers = n_layers, dropout=dropout, bidirectional=True)\n",
    "            self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "            self.num_directions = 2\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # -- COMMENT\n",
    "    # -- The src means sourse, which is a 2d array batch_size by sentence_len, it is a big 2d tensor\n",
    "    # -- NBBBBB: How to turn the POSTaggedDataset into a big 2d tensor see test_p2 line 4-7\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "\n",
    "        :param src: a [batch_size, sentence_len] array.\n",
    "                     Each row is a sequence of word indices and each column represents a position in the sequence.\n",
    "        :return: the predicted logits at each position. \n",
    "        \"\"\"\n",
    "\n",
    "        emb = self.embedding(src)\n",
    "        output = self.rnn(emb)[0]\n",
    "        logit = self.fc(output)\n",
    "        logit = torch.swapaxes(logit, 1, 2)\n",
    "        weight_vector = self.vector_weights[src]\n",
    "        weight_vector = torch.softmax(weight_vector.unsqueeze(dim=2),dim=1)\n",
    "        \n",
    "        return torch.bmm(logit,weight_vector).squeeze(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:06<00:00, 58116.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 91744.85it/s]\n",
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 400000\n",
      "Number of test sentences = 100000\n",
      "Number of unique input tokens = 340059\n",
      "Maximal sentence length = 226\n",
      "\n",
      "\n",
      " Creating training Dataset, Sampler, and Iterators...\n",
      "\n",
      "\n",
      " Creating test Dataset, Sampler, and Iterators\n",
      "Training first batch max length = 95\n",
      "Training second batch max length = 68\n",
      "Training last batch max length = 9\n",
      "Training second last batch max length = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-jiz322/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "corpora = Corpora()\n",
    "\n",
    "corpora.read_corpus(True)\n",
    "corpora.read_corpus(False)\n",
    "\n",
    "print(f'Number of training sentences = {len(corpora.training_reviews)}')\n",
    "print(f'Number of test sentences = {len(corpora.test_reviews)}')\n",
    "print(f'Number of unique input tokens = {len(corpora.word_index)}')\n",
    "print(f'Maximal sentence length = {corpora.max_len}')\n",
    "\n",
    "print(\"\\n\\n Creating training Dataset, Sampler, and Iterators...\")\n",
    "training_dataset = ReviewRateDataset(corpora.training_reviews)\n",
    "training_sampler = SortedBatchSampler(training_dataset, batch_size=BATCH_SIZE)\n",
    "training_iterator = DataLoader(training_dataset,\n",
    "                                  collate_fn = padding_collate_func,\n",
    "                                  batch_sampler = training_sampler)\n",
    "print(\"\\n\\n Creating test Dataset, Sampler, and Iterators\")\n",
    "test_dataset = ReviewRateDataset(corpora.test_reviews)\n",
    "test_sampler = SortedBatchSampler(test_dataset, batch_size=BATCH_SIZE)\n",
    "test_iterator = DataLoader(test_dataset,\n",
    "                              collate_fn = padding_collate_func,\n",
    "                              batch_sampler = test_sampler)\n",
    "\n",
    "print(f'Training first batch max length = {len(list(training_sampler)[0][0][0])}')\n",
    "print(f'Training second batch max length = {len(list(training_sampler)[1][0][0])}')\n",
    "print(f'Training last batch max length = {len(list(training_sampler)[-1][0][0])}')\n",
    "print(f'Training second last batch max length = {len(list(training_sampler)[-2][0][0])}')\n",
    "\n",
    "\n",
    "\n",
    "INPUT_DIM = len(corpora.word_index)\n",
    "OUTPUT_DIM = 5\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 5\n",
    "N_LAYERS = 1 # number of LSTM layers.\n",
    "BIDIRECT = 0 # 0: single direction (the default setting); 1: bidirectional\n",
    "DROPOUT = 0.5\n",
    "# initialize the model\n",
    "ScoreAssigner = LSTMScoreAssigner(INPUT_DIM, OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECT).cuda(3)\n",
    "\n",
    "\n",
    "\n",
    "# Glove Embedding here?\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "ScoreAssigner.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(ScoreAssigner.parameters())\n",
    "\n",
    "# we use 0 to represent padded POS tags and the loss function should ignore that.\n",
    "# we calculate the sum of losses of pairs in each batch\n",
    "PAD_INDEX = 0\n",
    "\n",
    "\n",
    "# input: vector of [length, output_dim], integer (score)\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum', ignore_index = PAD_INDEX)\n",
    "\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "training_losses = []\n",
    "test_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -- The Iterator is a Dataloader object. \n",
    "# -- Use for loop in iterator.batch_sampler to access each batches\n",
    "# -- In this case, each batches is having length 128\n",
    "\n",
    "# -- Need to Figure out: The way to compute loss for RNN\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total = 0\n",
    "\n",
    "    # batch[0]: the word batch\n",
    "    # batch[1]: the tag batch (target)\n",
    "    for i, batch in enumerate(iterator.batch_sampler):\n",
    "        #skip first batch\n",
    "        if i == 0:\n",
    "            continue\n",
    "        num_batchs += 1\n",
    "        z = ScoreAssigner.forward(batch[0].cuda(3))\n",
    "        #a = torch.softmax(z,dim=-1)\n",
    "        loss = 0\n",
    "        # softmax of logit\n",
    "        d = torch.softmax(z,dim=-1)\n",
    "        # cross entropy loss of softmax and score\n",
    "        loss=criterion(d,(batch[1]-1).cuda(3))/BATCH_SIZE\n",
    "        loss.backward()\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return epoch_loss /total\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    num_epochs = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator.batch_sampler):\n",
    "        #skip first batch\n",
    "        if i == 0:\n",
    "            continue\n",
    "        num_epochs += 1\n",
    "        z = ScoreAssigner.forward(batch[0].cuda(3))\n",
    "        loss = 0\n",
    "        # softmax of logit\n",
    "        d = torch.softmax(z,dim=-1)\n",
    "        # cross entropy loss of softmax and score\n",
    "        loss=criterion(d,(batch[1]-1).cuda(3))/BATCH_SIZE\n",
    "        epoch_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return epoch_loss/total\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start:  0\n",
      "Epoch: 01 | Time: 2m 25s\tTrain Loss: 1.213 | Test Loss: 1.153\n",
      "epoch start:  1\n",
      "Epoch: 02 | Time: 2m 25s\tTrain Loss: 1.100 | Test Loss: 1.161\n",
      "epoch start:  2\n",
      "Epoch: 03 | Time: 2m 25s\tTrain Loss: 1.069 | Test Loss: 1.128\n",
      "epoch start:  3\n",
      "Epoch: 04 | Time: 2m 25s\tTrain Loss: 1.049 | Test Loss: 1.112\n",
      "epoch start:  4\n",
      "Epoch: 05 | Time: 2m 25s\tTrain Loss: 1.033 | Test Loss: 1.116\n",
      "epoch start:  5\n",
      "Epoch: 06 | Time: 2m 25s\tTrain Loss: 1.020 | Test Loss: 1.124\n",
      "epoch start:  6\n",
      "Epoch: 07 | Time: 2m 25s\tTrain Loss: 1.011 | Test Loss: 1.128\n",
      "epoch start:  7\n",
      "Epoch: 08 | Time: 2m 25s\tTrain Loss: 1.001 | Test Loss: 1.118\n",
      "epoch start:  8\n",
      "Epoch: 09 | Time: 2m 25s\tTrain Loss: 0.993 | Test Loss: 1.106\n",
      "epoch start:  9\n",
      "Epoch: 10 | Time: 2m 25s\tTrain Loss: 0.990 | Test Loss: 1.110\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):  \n",
    "    print(\"epoch start: \", epoch)  \n",
    "    start_time = time.time()\n",
    "    training_loss = train(ScoreAssigner, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    test_loss = evaluate(ScoreAssigner, test_iterator, criterion)\n",
    "    test_losses.append(test_loss)  \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(ScoreAssigner.state_dict(), 'best_model.pt')\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "import pickle\n",
    "with open(f'results/losses_L{N_LAYERS}_D{DROPOUT}_B{BIDIRECT}.pkl', 'wb') as f:\n",
    "    pickle.dump({'training_losses': training_losses,\n",
    "                'test_losses': test_losses}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2cbb9814d0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n",
    "x = np.arange(len(training_losses))\n",
    "plt.plot(x, training_losses, label = 'training loss')\n",
    "plt.plot(x, test_losses, label = 'test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"best\"\n",
    "example2 = \"good\"\n",
    "example3 = \"okay\"\n",
    "example4 = \"trash\"\n",
    "example_s1 = \"it definitely worth its price. will purchase again. cheap and of good quality.\"\n",
    "example_s2 = \"it is very a peice of trash. it is too expensive comparing with other options.\"\n",
    "\n",
    "words1 = example1.split(\" \")\n",
    "words2 = example2.split(\" \")\n",
    "words3 = example3.split(\" \")\n",
    "words4 = example4.split(\" \")\n",
    "words5 = example_s1.split(\" \")\n",
    "words6 = example_s2.split(\" \")\n",
    "\n",
    "seq1 = []\n",
    "seq2 = []\n",
    "seq3 = []\n",
    "seq4 = []\n",
    "seq5 = []\n",
    "seq6 = []\n",
    "for word in words1:\n",
    "    seq1.append(corpora.word_index[word])\n",
    "for word in words2:\n",
    "    seq2.append(corpora.word_index[word])\n",
    "for word in words3:\n",
    "    seq3.append(corpora.word_index[word])\n",
    "for word in words4:\n",
    "    seq4.append(corpora.word_index[word])\n",
    "for word in words5:\n",
    "    seq5.append(corpora.word_index[word])\n",
    "for word in words6:\n",
    "    seq6.append(corpora.word_index[word])\n",
    "\n",
    "\n",
    "seq1 = torch.tensor([seq1]).cuda(3)\n",
    "seq2 = torch.tensor([seq2]).cuda(3)\n",
    "seq3 = torch.tensor([seq3]).cuda(3)\n",
    "seq4 = torch.tensor([seq4]).cuda(3)\n",
    "seq5 = torch.tensor([seq5]).cuda(3)\n",
    "seq6 = torch.tensor([seq6]).cuda(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  best \n",
      "prediction:  tensor([[1.9940e-24, 2.4509e-25, 6.3994e-24, 2.5246e-08, 1.0000e+00]],\n",
      "       device='cuda:3', grad_fn=<SoftmaxBackward0>)\n",
      "Example:  good \n",
      "prediction:  tensor([[2.6509e-21, 3.5353e-25, 5.7979e-13, 1.0000e+00, 1.2645e-14]],\n",
      "       device='cuda:3', grad_fn=<SoftmaxBackward0>)\n",
      "Example:  okay \n",
      "prediction:  tensor([[1.2399e-13, 1.3042e-06, 9.9509e-01, 4.9040e-03, 1.6301e-13]],\n",
      "       device='cuda:3', grad_fn=<SoftmaxBackward0>)\n",
      "Example:  trash \n",
      "prediction:  tensor([[7.3998e-20, 2.4821e-04, 9.9975e-01, 5.5537e-18, 1.2741e-25]],\n",
      "       device='cuda:3', grad_fn=<SoftmaxBackward0>)\n",
      "Example:  it definitely worth its price. will purchase again. cheap and of good quality. \n",
      "prediction:  tensor([[4.4822e-11, 7.6189e-04, 3.6215e-02, 9.6283e-01, 1.8846e-04]],\n",
      "       device='cuda:3', grad_fn=<SoftmaxBackward0>)\n",
      "Example:  it is very a peice of trash. it is too expensive comparing with other options. \n",
      "prediction:  tensor([[4.9861e-12, 8.4658e-01, 4.2018e-06, 4.8009e-05, 1.5337e-01]],\n",
      "       device='cuda:3', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = ScoreAssigner.forward(seq1)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example1, \"\\nprediction: \", d)\n",
    "z = ScoreAssigner.forward(seq2)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example2, \"\\nprediction: \", d)\n",
    "z = ScoreAssigner.forward(seq3)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example3, \"\\nprediction: \", d)\n",
    "z = ScoreAssigner.forward(seq4)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example4, \"\\nprediction: \", d)\n",
    "z = ScoreAssigner.forward(seq5)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example_s1, \"\\nprediction: \", d)\n",
    "z = ScoreAssigner.forward(seq6)\n",
    "d = torch.softmax(z,dim=-1)\n",
    "print(\"Example: \", example_s2, \"\\nprediction: \", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never',\n",
       " 'even',\n",
       " 'just',\n",
       " 'anything',\n",
       " 'change',\n",
       " 'found',\n",
       " 'were',\n",
       " \"won't\",\n",
       " 'wont',\n",
       " 'can',\n",
       " 'no',\n",
       " 'very',\n",
       " 'stay',\n",
       " \"it's\",\n",
       " 'much',\n",
       " 'battery',\n",
       " 'same',\n",
       " 'wore',\n",
       " 'didnt',\n",
       " 'months']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20 = torch.argsort(ScoreAssigner.vector_weights)[0:20]\n",
    "top20_words = []\n",
    "for i in top20:\n",
    "    for j in corpora.word_index.keys():\n",
    "        if corpora.word_index[j] == i:\n",
    "            top20_words.append(j)\n",
    "            \n",
    "top20_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
