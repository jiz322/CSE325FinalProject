{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26374"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 40000\n",
      "('5', '\"I bought Tourmaline flat iron for my daughter in law for Christmas and they were on sale so I bought me one too. I couldn\\'t believe the price. When I used it for the first time, my husband even noticed the difference. I was amazed how it made my hair feel as well as look. My daughter in law loves it as well. She\\'s the one who made me get it in the first place. I would definitely recommend it above any flat irons I\\'ve ever owned and I purchased it at the right time.\"')\n",
      "['5', '4', '3', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "list2 = []\n",
    "file_data = open('amazon_review_50000.csv')\n",
    "for row in file_data:\n",
    "    list2.append(row)## Data pre-processing module\n",
    "    \n",
    "list3 = []\n",
    "for i in range(len(list2)):\n",
    "    list3.append((list2[i][-2],list2[i][:-3]))\n",
    "    \n",
    "lenth = len(list3)\n",
    "train_list = list3[0:int(lenth*0.8)]\n",
    "test_list = list3[int(lenth*0.8):]\n",
    "print(len(test_list), len(train_list))\n",
    "print(train_list[3000])\n",
    "\n",
    "l = []\n",
    "for i,j in list3:\n",
    "    if i not in l:\n",
    "        l.append(i)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED=4321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "    The class holds training and test corpora.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        # word to index (1-based integers) mapping\n",
    "        self.word_index = {\"N0O0N\":0}\n",
    "        # list of reviews tuples, each of which is (sentence_list, rate),\n",
    "        self.training_reviews = []\n",
    "        # (sentence_list, rate) Same format as training_sentences\n",
    "        self.test_reviews = []\n",
    "\n",
    "        self.max_len = 0\n",
    "\n",
    "\n",
    "    # input: a tuple (reviewList, rate)\n",
    "    # todo: insert values into fields\n",
    "    # Return the list representing all index of words in a review.\n",
    "    def insert_fields(self, input):   \n",
    "        # Sentence list\n",
    "        word_indexes = []\n",
    "        for word in input:\n",
    "            if word not in self.word_index.keys():\n",
    "                self.word_index.update({word:len(self.word_index.keys())}) #No add 1 because 0 is already in\n",
    "            # find the index of this word, add to return list\n",
    "            word_indexes.append(self.word_index[word])\n",
    "        if len(word_indexes)>self.max_len:\n",
    "            self.max_len = len(word_indexes)\n",
    "        return word_indexes\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Different than P2, here we \n",
    "    def read_corpus(self, is_training):\n",
    "        if is_training is True:\n",
    "            target = train_list\n",
    "        else:\n",
    "            target = test_list\n",
    "        print(\"reading corpus ...\")\n",
    "        for rate, text in tqdm(target):\n",
    "            input = text.split(\" \")\n",
    "            tuple = (self.insert_fields(input), rate)\n",
    "            if is_training: \n",
    "                self.training_reviews.append(tuple)\n",
    "            else:\n",
    "                self.test_reviews.append(tuple)\n",
    "                    \n",
    "                \n",
    "# Inherient Dataset, convert list and int to tensors, load to GPU.\n",
    "class ReviewRateDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, review_rate_pairs): # NB: sequence_pairs is corpora.training_reviews, \n",
    "        # list of (sentence_list, rate)\n",
    "        self.review_rate_pairs = review_rate_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review_rate_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_list, rate = self.review_rate_pairs[idx] \n",
    "        return torch.tensor(sentence_list), torch.tensor(int(rate))\n",
    "\n",
    "# NB! This class will be in DataLoader function as a parameter for batch_sampler\n",
    "class SortedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "        Each sequence in a mini-batch must of the same lengths, while our sentences\n",
    "        are of various lengths.\n",
    "        We can pad the sentences to the same lengths in each mini-batch.\n",
    "        But if a short and long sentences are in the same mini-batch, more paddings\n",
    "        are needed.\n",
    "        We sort the sentences based on their lengths (in descending order)\n",
    "            and then put sentences with similar lengths in a batch to reduce the paddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "            dataset: an torch.utils.data.DataSet object containing all training sequences\n",
    "            batch_size: the number of sequences to put in a mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # The sorting and batching go within this function.      \n",
    "        self.batch_size = batch_size \n",
    "        # Sort the dataset (Based on the length of sentence.)\n",
    "        dataset.review_rate_pairs  = sorted(dataset.review_rate_pairs,key=lambda x:len(x[0]), reverse=True)\n",
    "        self.sorted_lengths = len(dataset)\n",
    "        # Batching: Split the dataset into a list of datasets\n",
    "        self.index_batches = []  \n",
    "        # -- NB: Collate function does not work, so I pad it directly.\n",
    "        for i in range(self.__len__()):\n",
    "            self.index_batches.append(padding_collate_func(ReviewRateDataset(dataset.review_rate_pairs[i*batch_size:i*batch_size+batch_size])))\n",
    "        # Now, each mini-batches is a ReviewRateDataset object\n",
    "        # If else format is needed, may change it latter.\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            return a Python iterator object that iterates the mini-batchs of\n",
    "                training data indices (not individual indices)\n",
    "        \"\"\"\n",
    "        return iter(self.index_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sorted_lengths // self.batch_size\n",
    "\n",
    "# NB! This function will be in DataLoader function as a parameter for collate_fn\n",
    "def padding_collate_func(batch):\n",
    "    \"\"\"\n",
    "        Transform pairs of input-output sequences in the batch to be of the same length using the function\n",
    "            torch.nn.utils.rnn.pad_sequence.\n",
    "        batch: An iterator and each element is a pair of (input_sequence, output_sequence).\n",
    "        For POS tagging, len(input_sequence) = len(output_sequence). But for different\n",
    "        pairs in batch, their lengths can differ.\n",
    "\n",
    "        Example: a batch of 3 pairs of input/output sequences\n",
    "                [([1,2,3],[1,1,1]), ([1,2,3,4],[2,2,2,2]), ([1,2,3,4,5],[3,3,3,3,3])]\n",
    "                Note: [] encloses tensors (not numpy arra ys)\n",
    "                \n",
    "                \n",
    "                !!!!!NB QUESTION:  it is the inner [] that encloses tensors, right?\n",
    "                Comment: Batch is an element of a Sampler (see test_p1.py, a l[0] is a batch)\n",
    "                \n",
    "                \n",
    "        return: two tensors (one for input sequence batch and another for output sequence batch).\n",
    "                These tensors are padded with zeros so that all sequences in the same batch\n",
    "                are of the same length.\n",
    "        Example: input_sequence_batch = [[1,2,3,0,0], [1,2,3,4,0], [1,2,3,4,5]],\n",
    "                 output_sequence_batch = [[1,1,1,0,0], [2,2,2,2,0], [3,3,3,3,3]]\n",
    "\n",
    "    \"\"\"\n",
    "    ### Your codes go here (5 points) ###\n",
    "    # Hint: read the article linked at the top of this cell.\n",
    "    \n",
    "    # NOTe\n",
    "    # len(batch[0][0]) == len(batch.sequence_pairs[0]) == the target value (the maximum length for each batch)\n",
    "    # I fill it might be easier to pad sequence_pairs with tuple (0,0), but let's see.\n",
    "    \n",
    "    # Doubles the memory takes by this batch, which is not good\n",
    "    new_l = []\n",
    "    rate = []\n",
    "    for i,j in batch.review_rate_pairs:\n",
    "        new_l.append( torch.tensor(i))\n",
    "        rate.append( torch.tensor(int(j)))\n",
    "    padded = pad_sequence(new_l, batch_first=True, padding_value=0)\n",
    "    #print(\"len \", len(padded), len(rate)) #BUG: NEED ZIP HERE\n",
    "    obj = ReviewRateDataset(list(zip(padded,rate)))\n",
    "    # NB: For now the outter [] is neither tensor nor list, it is an obj!\n",
    "    # Change latter if necessary\n",
    "    # Yes, let's change it to two tensors return\n",
    "    ret1 = []\n",
    "    ret2 = []\n",
    "    for i in obj:\n",
    "        ret1.append(i[0])\n",
    "        ret2.append(i[1])\n",
    "    ret1 = torch.stack(ret1)\n",
    "    ret2 = torch.stack(ret2)\n",
    "    return ret1, ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import embedding, nn\n",
    "# There is really nothing to be stored in this object.\n",
    "# -- But wait, how about self.rnn and self.fc?\n",
    "# -- NB: NOW, I assume that the nn keep weights from the inherentance,\n",
    "# -- And these functions as LSTM and FC will use these weight correctly\n",
    "class LSTMScoreAssigner(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        \"\"\"\n",
    "        :param input_dim: size of the vocabulary (number of unique tokens)\n",
    "        :param output_dim: number of unique POS tags \n",
    "        :param emb_dim: embedding dimensionality of each token\n",
    "        :param hid_dim: number of hidden neurons of a hidden state/cell\n",
    "        :param n_layers: number of RNN layers (2 for faster training)\n",
    "        :param dropout: dropout rate between 0 and 1at the embedding layer and rnn\n",
    "        :param bidirectional: 1 if use bidirectional and 0 if don't\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # before output, there is a dropout (except the last layer)\n",
    "        \n",
    "        # -- Comment: this is a part of the analysis (last part of this hw)\n",
    "        # -- It feels like I have no control on this bidirectional since it is part of nn library.\n",
    "        if bidirectional == 0:\n",
    "            self.rnn = nn.LSTM(input_size = emb_dim, hidden_size = hid_dim, num_layers = n_layers, dropout=dropout)\n",
    "            self.fc = nn.Linear(hid_dim, output_dim)\n",
    "            self.num_directions = 1\n",
    "        elif bidirectional == 1:\n",
    "            self.rnn = nn.LSTM(input_size = emb_dim, hidden_size = hid_dim, num_layers = n_layers, dropout=dropout, bidirectional=True)\n",
    "            self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "            self.num_directions = 2\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # -- COMMENT\n",
    "    # -- The src means sourse, which is a 2d array batch_size by sentence_len, it is a big 2d tensor\n",
    "    # -- NBBBBB: How to turn the POSTaggedDataset into a big 2d tensor see test_p2 line 4-7\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "\n",
    "        :param src: a [batch_size, sentence_len] array.\n",
    "                     Each row is a sequence of word indices and each column represents a position in the sequence.\n",
    "        :return: the predicted logits at each position. \n",
    "        \"\"\"\n",
    "        # -- src : a list of sentence tensors\n",
    "        # -- logit : a tensor having length of self.output_dim\n",
    "        \n",
    "        \n",
    "        ### Your codes go here (20 points) ###\n",
    "\n",
    "        # Step 1: turn token indices into dense vector,\n",
    "        # so that embedded is of shape (batch_size, sentence_len, emb_dim)\n",
    "        src = self.embedding(src)\n",
    "        # Step 2: rnn maps the tensor (batch_size, sentence_len, emb_dim) to\n",
    "        # outputs = a tensor (batch_size, sentence_len, hid_dim)\n",
    "        # hidden = a tensor (batch_size, sentence_len, hid_dim)\n",
    "        # cell = a tensor (batch_size, sentence_len, hid_dim)\n",
    "        \n",
    "        # See library LSTM to continuw\n",
    "        # Maybe find examples of LSTM\n",
    "        \n",
    "        # # Construct a h_0 and c_0\n",
    "        # h_0 = 0\n",
    "        # c_0 = 0\n",
    "        \n",
    "        # outputs, (hidden, cell) = self.rnn(self.embedding(src, (h_0, c_0)))\n",
    "        # --- Come back from office hour\n",
    "        output = self.rnn(src)[0]\n",
    "        # The  self.rnn(src)[1] is a tuple of (h_n, c_n)\n",
    "        # -- Think: is c_0 and h_0 necessary? I guess no for now.\n",
    "\n",
    "        # Step 3: map the output tensor to a logit tensor of shape (batch_size, sentence_len, number_of_POS_tags)\n",
    "        logit = self.fc(output)\n",
    "        return torch.sum(logit, dim=1)/logit.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 44450.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 50209.36it/s]\n",
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 40000\n",
      "Number of test sentences = 10000\n",
      "Number of unique input tokens = 114084\n",
      "Maximal sentence length = 1379\n",
      "\n",
      "\n",
      " Creating training Dataset, Sampler, and Iterators...\n",
      "\n",
      "\n",
      " Creating test Dataset, Sampler, and Iterators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-jiz322/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "corpora = Corpora()\n",
    "\n",
    "corpora.read_corpus(True)\n",
    "corpora.read_corpus(False)\n",
    "\n",
    "print(f'Number of training sentences = {len(corpora.training_reviews)}')\n",
    "print(f'Number of test sentences = {len(corpora.test_reviews)}')\n",
    "print(f'Number of unique input tokens = {len(corpora.word_index)}')\n",
    "print(f'Maximal sentence length = {corpora.max_len}')\n",
    "\n",
    "print(\"\\n\\n Creating training Dataset, Sampler, and Iterators...\")\n",
    "training_dataset = ReviewRateDataset(corpora.training_reviews)\n",
    "training_sampler = SortedBatchSampler(training_dataset, batch_size=BATCH_SIZE)\n",
    "training_iterator = DataLoader(training_dataset,\n",
    "                                  collate_fn = padding_collate_func,\n",
    "                                  batch_sampler = training_sampler)\n",
    "print(\"\\n\\n Creating test Dataset, Sampler, and Iterators\")\n",
    "test_dataset = ReviewRateDataset(corpora.test_reviews)\n",
    "test_sampler = SortedBatchSampler(test_dataset, batch_size=BATCH_SIZE)\n",
    "test_iterator = DataLoader(test_dataset,\n",
    "                              collate_fn = padding_collate_func,\n",
    "                              batch_sampler = test_sampler)\n",
    "\n",
    "\n",
    "INPUT_DIM = len(corpora.word_index)\n",
    "OUTPUT_DIM = 5\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 5\n",
    "N_LAYERS = 1 # number of LSTM layers.\n",
    "BIDIRECT = 0 # 0: single direction (the default setting); 1: bidirectional\n",
    "DROPOUT = 0.5\n",
    "# initialize the model\n",
    "ScoreAssigner = LSTMScoreAssigner(INPUT_DIM, OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECT).cuda(3)\n",
    "\n",
    "\n",
    "\n",
    "# Glove Embedding here?\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "ScoreAssigner.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(ScoreAssigner.parameters())\n",
    "\n",
    "# we use 0 to represent padded POS tags and the loss function should ignore that.\n",
    "# we calculate the sum of losses of pairs in each batch\n",
    "PAD_INDEX = 0\n",
    "\n",
    "\n",
    "# input: vector of [length, output_dim], integer (score)\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum', ignore_index = PAD_INDEX)\n",
    "\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "training_losses = []\n",
    "test_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -- The Iterator is a Dataloader object. \n",
    "# -- Use for loop in iterator.batch_sampler to access each batches\n",
    "# -- In this case, each batches is having length 128\n",
    "\n",
    "# -- Need to Figure out: The way to compute loss for RNN\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total = 0\n",
    "\n",
    "    # batch[0]: the word batch\n",
    "    # batch[1]: the tag batch (target)\n",
    "    for i, batch in enumerate(iterator.batch_sampler):\n",
    "        num_batchs += 1\n",
    "        z = ScoreAssigner.forward(batch[0].cuda(3))\n",
    "        #a = torch.softmax(z,dim=-1)\n",
    "        loss = 0\n",
    "        # softmax of logit\n",
    "        d = torch.softmax(z,dim=-1)\n",
    "        # cross entropy loss of softmax and score\n",
    "        loss=criterion(d,(batch[1]-1).cuda(3))/BATCH_SIZE\n",
    "        loss.backward()\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return epoch_loss /total\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    num_epochs = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator.batch_sampler):\n",
    "        num_epochs += 1\n",
    "        z = ScoreAssigner.forward(batch[0].cuda(3))\n",
    "        loss = 0\n",
    "        # softmax of logit\n",
    "        d = torch.softmax(z,dim=-1)\n",
    "        # cross entropy loss of softmax and score\n",
    "        loss=criterion(d,(batch[1]-1).cuda(3))/BATCH_SIZE\n",
    "        epoch_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return epoch_loss/total\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start:  0\n",
      "Epoch: 01 | Time: 0m 24s\tTrain Loss: 1.159 | Test Loss: 1.138\n",
      "epoch start:  1\n",
      "Epoch: 02 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  2\n",
      "Epoch: 03 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  3\n",
      "Epoch: 04 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  4\n",
      "Epoch: 05 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  5\n",
      "Epoch: 06 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  6\n",
      "Epoch: 07 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  7\n",
      "Epoch: 08 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  8\n",
      "Epoch: 09 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n",
      "epoch start:  9\n",
      "Epoch: 10 | Time: 0m 21s\tTrain Loss: 1.139 | Test Loss: 1.138\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):  \n",
    "    print(\"epoch start: \", epoch)  \n",
    "    start_time = time.time()\n",
    "    training_loss = train(ScoreAssigner, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    test_loss = evaluate(ScoreAssigner, test_iterator, criterion)\n",
    "    test_losses.append(test_loss)  \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(ScoreAssigner.state_dict(), 'best_model.pt')\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "import pickle\n",
    "with open(f'results/losses_L{N_LAYERS}_D{DROPOUT}_B{BIDIRECT}.pkl', 'wb') as f:\n",
    "    pickle.dump({'training_losses': training_losses,\n",
    "                'test_losses': test_losses}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdba0f760d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n",
    "x = np.arange(len(training_losses))\n",
    "plt.plot(x, training_losses, label = 'training loss')\n",
    "plt.plot(x, test_losses, label = 'test loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
